SASRec是属于序列推荐里面的经典模型，由于之前并没有怎么接触过推荐系统，所以就想要尝试复现论文并进行一些实验加深对序列推荐人物的理解。
当前仓库主要借鉴以下仓库：
1. https://github.com/pmixer/SASRec.pytorch/tree/main
2. https://github.com/JamZheng/CL4SRec-pytorch/tree/main

相关论文参考：
1. https://doi.org/10.48550/arXiv.1808.09781
2. https://doi.org/10.48550/arXiv.2010.14395
 


## TODO:
- [x] 添加对比学习
- [x] 测试分析不同对比学习方式的效果
- [ ] 进一步优化

### 2025.4.24
今天开始阅读论文，大致完成了论文到训练细节之前，接下来是一些总结：
1. 论文主要贡献：提出一种基于自注意力机制的序列推荐模型SASRec（看年份感觉就是Transformer出来之后直接就套壳了）
2. 论文的motivation：前序关于序列推荐的算法主要是基于RNN的和基于马尔可夫链的，前者成本高，训练数据要求大（但是似乎没有具体在论文的前半部分具体论证），后者基于简单的强假设：每一个行为都仅取决于少数前几个行为（有点类似状态转移），就过于局部了。文章就想要找一个balance的方法，既可以考虑全局的信息，又可以考虑局部的信息。
3. 关于推荐算法：因为之前并不熟这个领域，所以想要稍微总结一下里面related work中的内容：
    1. 一般来说推荐算法就是建模用户与物品之间的关系，然后可利用的信息一般就是隐性或显性的历史反馈，对于前者而言建模相对比较困难，因为该反馈仅界定了正样本，而负样本没有被界定，比如：我点击了手机商品，最后买了平板，但是对于购买的历史动作而言，没有明确的表示我点击了平板。
    2. 结合商品的其他属性信息似乎叫content-aware recommendation
    3. 常见方法有矩阵分解的方法：利用内积建模用户与物品之间的交互。也有基于相似度的方法：利用余弦相似度或欧氏距离建模物品与物品之间的相似。
4. 论文方法：感觉模型没有和transformer有很大区别，目前有区别的几个点：1、positional embedding说是用transformer的表现不好。2、Q\K切断联系？（这个有点没明白，猜测是用mask的方式）

然后实践方面目前先把数据下下来，今天给数据处理开个头。

### 2025.4.25
目前已经完成了初步的预处理，因为原文有提供一个数据集，所以我现在正在尝试修改原文预处理方法去匹配上这个数据集
但是目前出现两个问题，使用仓库作者的阈值3会导致数据量变大，而以5为阈值则和提供数据的数据量（行数）一致。
第二个问题就是目前用户和物品id的排序是乱的，我估计是数据在做重新排序之前已经先根据时间戳排过序了，所以现在实验一下
现在第二个实验出来还是不一样，但是通过一系列断言验证发现两个序列的统计属性、分布和化作图之后的结构都是一致的，暂时先认为两个序列是一致的。

现在进行模型搭建。
在实现的时候现在遇到了第一个问题，官方在计算自注意力机制的的时候和论文里的公式并不一致，官方在计算的时候是只对Q做了normalize，最后残差连接也是用这个Q去连接，但是原文是对输入做normalize，残差连接是连回原先的输入，所以这里打算做个实验，两个都试一下：
1. 官方实现版本：将Qnormalize，残差连接用Q
2. 原文实现版本：将输入normalize，残差连接用输入

然后大致是自己写了一遍这个版本，还没进行测试，目前学习到了一些东西如下：
1. 实际上这个attention还是用于建模用户的，就是一个用户行为的建模，最后的结果还是通过用户的行为特征和物品特征做内积交互得到
2. 但是这里训练时候用的正样本和负样本实际上有点没明白是什么，所以需要进一步弄明白来
目前初步测试模型可以跑通，但是还是需要等待进一步验证

### 2025.4.27
今天首先完成评估指标部分的代码
论文提及的评估指标有两个：
1. Hit Rate@10： 我的理解的话就是预测的top10中是否包含了正样本
2. NDCG@10： 这个就是和排名相关了，如果预测的排名越靠前就越好
论文里也提到了实施的方案，就是将ground truth和随机抽取的100个负样本组成样本，然后获得这些样本的预测结果的排序

在实践中遇到了一个没发现过的报错，记录一下：
表现：在运行的时候突然蹦出来：C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\native\cuda\Indexing.cu:1289: block: [3,0,0], thread: [2,0,0] Assertion `srcIndex < srcSelectDimSize` failed.  其中这里的线程和块的号有很多个都出了问题，这里只列出了其中一个
RuntimeError: CUDA error: device-side assert triggered  这个是最后的报错内容
根据内容首先需要定位具体报错具体位置，因为cuda编程是异步操作，首先还是使用cuda，但是设置环境变量CUDA_LAUNCH_BLOCKING为1去尝试同步纠错。
但是仍然出现问题，那么首先将设备切换为cpu查看是否出错。
仍然出错，但是这一次出现了显著报错，发现是按照仓库代码进行修改后里面有一个地方给模型传入了numpy数组，而不是tensor，所以需要修改。修改后运行通过。

完成评估指标部分的代码后，开始模型训练框架部分的代码编写。因为之前没有接触过推荐算法的训练流程，所以需要学习一下。
1. 第一步还是一样先处理数据，这里的数据划分就是将之前处理好的数据集按照用户聚合，但是似乎和之前的任务的数据划分不一样，这里并不按照用户进行划分，而是将用户前n-2个行为作为训练，第n-1个行为作为验证，第n个行为作为测试，也就是基于时间划分的方式。这个需要学习一下。
然后这里实现是用一个warpsampler去构造数据抽样，这里我就直接用torch的dataset和dataloader进行替换。
2. 第二步就是模型训练，这里的流程还是基本上是一致的。

目前代码已经可以正常运行了，尝试进行优化：
1. 对比学习：目前找到一篇文章发表于22ICDE上的，这个文章中的方案我个人感觉会比替换好一些，因为感觉替换的话总是感觉会引入比较大的噪声，并且该研究也有相关代码可以借鉴：https://github.com/JamZheng/CL4SRec-pytorch/blob/main/
2. 多任务学习：这个暂时只有一点思路，就是利用rating信息作为另一个预测任务，但是具体该怎么该模型还需要先想一下。
3. 结合其它信息优化： 这里我想到的就是将rating像positional embedding一样，加入到模型中，然后再进行训练。其它信息的利用就需要先看一下相关content_aware recommendation的论文了。


训练的时候发现了一个很不好的问题，就是使用datalodaer之后训练速度会慢很多，判断是因为dataloader每次都是一个一个去加载的，而官方实现是直接多线程预加载的，导致官方实现一个epoch只需要1-2s，而我的实现需要30s，所以现在试一试换上官方方法
并且现在发现效果也差很多，官方的HDCG @ 10只需要20epoch可以到差不多0.7，而我的只有0.1左右，但是实际上损失的降低是差不多的，感觉可能是评估方式写的有问题，现在一点一点排查
- [x] Dataloader替换官方实现WrapSampler  -->  训练速度得到显著提升，但是指标仍然很低，进一步排查
- [x] 官方评估实现与我的评估实现对比 --> 发现实现上出了我的版本是官方实现版本的合并之外似乎没有什么不一样的
- [x] 排查调用  -->  果然发现问题，有点太蠢了，将传入的参数顺序写反了QAQ，现在更改过来就发现好了很多，这些就是正确了


### 2025.4.28
今天主要采取对比学习的方式在模型基础上进行改进，参照的文献为Contrastive Learning for Sequential Recommendation，发表于ICDE 2022（看了一下似乎是CCF-A类会议，但是似乎主要方向是数据库的？有点奇怪）
这里主要流程只需要增加一个增强模块和一个对比学习的损失即可，损失的正例是两个增强的序列，负例是不同用户之间的增强序列。
其中增强方式在文中提及有三种，分别是：
1. 随机裁剪： 就是裁出序列的一个子序列，这个的好处是相对于替换来说可以保留原数据信息，可以提供一个局部的用户信息，在论文中提到如果两个没交集的子序列出现在对比学习的正例中，就需要模型学习出用户的局部变动（似乎还是很有道理的）
2. 物品mask：这个感觉也很符合直觉，像之前学习的BRET就有类似的预训练方式，而且相对于随机替换而言感觉这种方式并不会引入错误信息，而且也很符合实际世界的数据来源。但是研究里面虽然论文说是分配一个[mask]，但是实际上还是就是变成0，这样和pad的embedding是一样的感觉会不会有点不妥？
3. 重排：我个人认为这个方式并不太适合于当前这个数据集，因为这个假设是现实生活中用户交互顺序会随比较多的未观测因素而变得比较灵活，但是对于电影这个类型的交互来说感觉更像是一个长时间的行为，所以这里先不去采用这种增强方式。

目前简单测试了两种对比学习方法，首先是随机裁剪的增强方法，发现性能相比没有增强的模型并没有提升，反而是下降了，并且随着裁剪的比例增加，性能越低暂时针对这个现象有以下分析：
因为在进行对比学习的时候是进行余弦相似度分析，以使得正例相近的视角来看，相当于是将一个元素错序的向量加上相同的位置编码进行余弦相似度的最大化优化，那么有以下两种情况：
1. 当位置编码强度远大于元素嵌入的强度时，相似度由位置编码决定，但是由于负例使用的同样的位置编码，所以不会成为最优解。
2. 当位置编码强度小于元素嵌入的强度时，此时相似度由元素嵌入决定，由于当crop比例越大，保留序列越多，重复序列越高，那么就可以转换为两个错序向量之间的余弦相似度最大化的问题，那么此时最优解将会使得每个元素之间的相似度最大化，但是要使得两个用户的序列之间的差异最大化，那么似乎也不会使每个元素相似度最大化。
总结感觉是不是就是这样的限制使得优化难度比较大导致的?后续尝试使用均值构建向量来排除位置差异导致的两个相似向量之间的明显差异。

对于mask操作来说，mask的概率越大，性能越好，并且当mask概率为0.8的时候超出了复现的没有使用对比学习的性能，但是因为提升幅度只有0.03，所以还是有可能是训练的随机性导致的，需要进一步去验证。当mask概率低的时候，因为用于对比的向量基本没有mask，所以相似度自然很高，这样为了优化目标，对比学习损失应该会使得不同用户之间的序列差异增大，因为数据集的密度相对还是很大的，所以用户之间的相似度实际上猜测并不会太高，所以低mask情况下的优化任务应该并不会太困难。同时在训练过程中观察到最后对比学习损失降为了0，个人认为这个现象还是合理的。

### 2025.4.29
今天首先是验证昨天提及的crop效果不好的现象猜想，这里使用mean将向量的seq_len维度进行压缩，然后进一步做对比学习。
确实通过实验发现效果相比之前并不做处理的效果好了很多，说明对于不同的增强方式而言，对比学习所需要的嵌入输入的处理是有所区别的。
而对于replace和mask的方式而言，似乎并没有使得原先的效果增加很多
在观察训练日志的时候发现，如mask和replace操作都是当p值比较大的时候损失会从大致是5开始逐步降低，而当p值逐渐减小的时候损失可能就是从0开始，这个现象感觉是合理的，因为当p越小，实际上增强的序列就越接近原序列，那么之间的差异就并不大了，损失也就相应会小。
而对于crop操作，随p值增加，在模型收敛（early stop）之前的对比学习损失降低的就越慢，当p为0.2时，最终对比学习损失可以降低至0，而当p比较大时可能最终只能停留在3左右。这个现象对应的就是提升上当p为0.2时提升最大。这里暂时没有一个很好的猜测，或许是不重叠的局部片段之间的分布相似可以进一步提升模型性能？


## result（采用的是SASRec的采样评估版本）
|   METHOD  | Hit @ 10 | HDCG @ 10   |    SPEED(但是似乎这个时间还是很有问题的，因为可能同时跑多个进程导致的记录的10个epoch的时间很震荡，这里记录的是log里面前几十个epoch的一个均值（大致）)    |
|  :----:   |  :----:  |   :----:    |   :----:    |
| SASRec    |  0.8245  |   0.5905    | 1.7s/epoch  |
| Reproduct |  0.8442  |   0.6182    |   4.2s/epoch    |
| Contrastive learning(crop->p=0.8) |  0.8161  |   0.5479    |   4.5s/epoch    |
| Contrastive learning(crop->p=0.6) |  0.8182  |   0.5511    |   5.1s/epoch    |
| Contrastive learning(crop->p=0.4) |  0.8275  |   0.5646    |   4.5s/epoch    |
| Contrastive learning(crop->p=0.2) |  0.8338  |   0.5710    |   3.7s/epoch    |
| Contrastive learning(mask->p=0.8) |  0.8474  |   0.6204    |   6.3s/epoch    |
| Contrastive learning(mask->p=0.6) |  0.8440  |   0.6137    |   6.6s/epoch    |
| Contrastive learning(mask->p=0.4) |  0.8386  |   0.6022    |   6.6s/epoch    |
| Contrastive learning(mask->p=0.2) |  0.8366  |   0.5992    |   3.8s/epoch    |
| Contrastive learning(replace->p=0.8) |  0.8136  |   0.5564    |   3.0s/epoch    |
| Contrastive learning(replace->p=0.6) |  0.8474  |   0.6188    |   3.8s/epoch    |
| Contrastive learning(replace->p=0.4) |  0.8411  |   0.6041    |   3.7s/epoch    |
| Contrastive learning(replace->p=0.2) |  0.8437  |   0.6157    |   5.5s/epoch    |
| Contrastive learning(crop->p=0.8)(mean) |  0.8419  |   0.6101    |   3.3s/epoch    |
| Contrastive learning(crop->p=0.6)(mean) |  0.8381  |   0.6068    |   4.7s/epoch    |
| Contrastive learning(crop->p=0.4)(mean) |  0.8452  |   0.6148    |   4.7s/epoch    |
| Contrastive learning(crop->p=0.2)(mean) |  0.8503  |   0.6236    |   5.5s/epoch    |
| Contrastive learning(mask->p=0.8)(mean) |  0.8439  |   0.6074    |   3.6s/epoch    |
| Contrastive learning(mask->p=0.6)(mean) |  0.8369  |   0.5988    |   3.7s/epoch    |
| Contrastive learning(mask->p=0.4)(mean) |  0.8470  |   0.6141    |   5.0s/epoch    |
| Contrastive learning(mask->p=0.2)(mean) |  0.8412  |   0.6087    |   3.8s/epoch    |
| Contrastive learning(replace->p=0.8)(mean) |  0.8411  |   0.6097    |   3.6s/epoch    |
| Contrastive learning(replace->p=0.6)(mean) |  0.8334  |   0.5914    |   4.3s/epoch    |
| Contrastive learning(replace->p=0.4)(mean) |  0.8439  |   0.6101    |   4.2s/epoch    |
| Contrastive learning(replace->p=0.2)(mean) |  0.8432  |   0.6117    |   4.7s/epoch    |